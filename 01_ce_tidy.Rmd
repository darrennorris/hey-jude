---
title: "01_ce_tidy"
author: "Darren Norris"
date: "`r Sys.Date()`"
output:
  bookdown::html_document2:
    toc: yes
    toc_float: yes
    toc_depth: 3
    fig_caption: yes
  bookdown::pdf_document2:
    toc: yes
    toc_depth: 3
    number_sections: yes
    extra_dependencies: flafter
    highlight: tango
    includes:
      in_header: preamble.txe
always_allow_html: yes
urlcolor: blue
toc-title: Contents
header-includes:
  - \counterwithin{figure}{section}
---

```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(
  echo = TRUE, collapse = TRUE,
  comment = "#>" 
  )
def_hook <- knit_hooks$get("output")
knit_hooks$set(output = function(x, options) {
  out <- def_hook(x, options)
  return(paste("\\begin{framed}\\begin{verbatim}", x, "\\end{verbatim}\\end{framed}", collapse = "\n"))
})
```

\newpage{}

In the end obtained 842 DOIS and 827 Abstracts for 1107 vertebrate articles from the Conservation Evidence database (total of 1145 studies including grey literature).
There are 49 without doi that should be available via Scopus (842 + 49 = 891).

## Packages
```{r load-packages, warning=FALSE, message=FALSE}
# Data processing and presentation
library(plyr) # plyr before tidyverse
library(tidyverse)
library(stringr)
library(textclean)
library(readxl)
library(gridExtra)
library(magrittr)
library(rcrossref)
library(rscopus)

```

## Load CE data
 CE https://www.conservationevidence.com/content/page/111 
 "Finding documented evidence" lists inclusion criteria
- There must have been an intervention that conservationists would do.
- Its effects must have been monitored quantitatively and documented.
Does not include studies that solely report monitoring methods, species ecology, biodiversity surveys, or threats to biodiversity.
```{r load-data}
# Has Conservation Evidence from the following synopses Amphibian, Bat, Bird, Management of captive animals, marine and freshwater mammals, marine fish, primates, reptiles, terrestrial mammals
# 2013 Bird
# 2014 Amphibian
# 2017 Primate
# 2018 Management of Captive Animals
# 2020 Terrestrial Mammal
# 2021 Bat, Reptile, Marine and Freshwater Mammal, Marine Fish
# Few Conservation Biology and Biodiversity and Conservation 
# 7874 - doi incorrect. Use manually adjusted dois
# 10316 - doi incorrect duplicate of 10478
# 8599 doi duplicate of 8598
# 10475 url duplicate of 10237

cedat <- read_excel('data/studydat_fordarrenn_24_02_2023DN.xlsx')
cedat %>% 
  # with 2011 lose 97 bird studies
  #filter(publication.year>=2011) %>%
  group_by(synopsis.name) %>% 
  summarise(study_count = length(unique(study.id)))

cedat %>% 
  # with 2011 lose 97 bird studies
  filter(synopsis.name=="Bird Conservation") %>%
  group_by(journal.name) %>% 
  summarise(study_count = length(unique(study.id))) %>% 
  arrange(desc(study_count))

cedat %>% 
  filter(publication.year>=2011) %>%
  # 1094 studies
  group_by(study.id, publication.year, publication.title, 
           journal.name,doi, url) %>% 
  summarise(intervention_count = n()) %>% 
  ungroup() -> cedat_studies
cedat_studies %>% nrow() #1094
# 35 not journals
cedat_studies %>% 
  filter(is.na(journal.name)) %>% nrow()
# 819 dois are NA
cedat_studies %>% 
  filter(is.na(doi)) %>% nrow()
# 275 dois not NA
cedat_studies %>% 
  filter(!is.na(doi)) %>% nrow()

cedat_studies %>% 
  filter(!is.na(journal.name)) %>% 
  group_by(journal.name) %>% summarise(article_count = n()) %>% 
  arrange(desc(article_count))

```

### Add birds
CE searched 3 March 2023. Filter birds and years 2011 - 2022 = 53 studies.
https://www.conservationevidence.com/data/studies?pp=100&broad_id%5B%5D=2&country%5B%5D=&result_type=references&sort=publication_year#searchcontainer

```{r}
library(synthesisr)
cebird <- read_refs(
  filename = "data/2023-03-03.ris",
  return_df = TRUE)

```


## Get dois from CE data.
```{r get-dois}
# Create new doi column to join with bibliometrics
# clean out prefixes
doi_prefix <- c("http://dx.doi.org/DOI:", "http://dx.doi.org/doi:", 
"http://dx.doi.org/", "https://dx.doi.org/DOI:", "https://dx.doi.org/doi:", 
"https://dx.doi.org/", 
"DOI:", "http://doi.org/", "https://doi.org/")
doi_prefix_string <- paste(doi_prefix,collapse="")

journal_doi_prefix <- c( "http://www.asmjournals.org/doi/abs/",
"http://link.springer.com/article/",  
"https://link.springer.com/chapter/", 
"https://link.springer.com/content/pdf/",
"http://www.bioone.org/doi/abs/", "http://www.karger.com/DOI/", 
"http://www.tandfonline.com/doi/abs/", 
"https://besjournals.onlinelibrary.wiley.com/doi/epdf/",
"https://besjournals.onlinelibrary.wiley.com/doi/abs/", 
"https://journals.plos.org/plosone/article/file?id=", 
"https://journals.plos.org/plosone/article?id=", 
"http://journals.sagepub.com/doi/abs/",
"https://link.springer.com/article/", 
"https://www.frontiersin.org/articles/",
"https://onlinelibrary.wiley.com/doi/epdf/",
"https://onlinelibrary.wiley.com/doi/pdf/",
"https://onlinelibrary.wiley.com/doi/abs/", 
"https://onlinelibrary.wiley.com/doi/full/", 
"https://onlinelibrary.wiley.com/doi/",
"http://onlinelibrary.wiley.com/doi/", 
"https://royalsocietypublishing.org/doi/pdf/",
"https://wildlife.onlinelibrary.wiley.com/doi/abs/", 
"https://wildlife.onlinelibrary.wiley.com/doi/pdf/",
"https://zslpublications.onlinelibrary.wiley.com/doi/epdf/",
"https://zslpublications.onlinelibrary.wiley.com/doi/pdf/",
"https://zslpublications.onlinelibrary.wiley.com/doi/", 
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997426723&doi=", 
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84951076741&doi=", 
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959282517&doi=", 
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84940942773&doi=", 
"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84949952613&doi="
)

cedat_studies %>% 
  mutate(doi = str_trim(doi), 
         url = str_trim(url)) %>%
  mutate(doi_ce = str_replace(doi, doi_prefix[1], "")) %>% 
  mutate(doi_ce = str_replace(doi_ce, doi_prefix[2], "")) %>% 
  mutate(doi_ce = str_replace(doi_ce, doi_prefix[3], "")) %>% 
  mutate(doi_ce = str_replace(doi_ce, doi_prefix[4], "")) %>% 
  mutate(doi_ce = str_replace(doi_ce, doi_prefix[5], "")) %>% 
  mutate(doi_ce = str_replace(doi_ce, doi_prefix[6], "")) %>% 
  mutate(doi_ce = str_replace(doi_ce, doi_prefix[7], "")) %>% 
  mutate(doi_ce = str_replace(doi_ce, doi_prefix[8], "")) %>% 
  mutate(doi_ce = str_replace(doi_ce, doi_prefix[9], "")) %>% 
  mutate(doi_ce = if_else(study.id ==7943 ,"10.1016/j.jnc.2015.03.007", doi_ce)) %>%
  mutate(doi_ce = str_replace(doi_ce, "http://www.bioone.org/doi/abs/", "")) %>% 
  mutate(doi_ce = str_replace(doi_ce, "https://link.springer.com/article/", "")) %>% 
  mutate(doi_ce = str_replace(doi_ce, "https://onlinelibrary.wiley.com/doi/abs/", "")) %>%  
  mutate(doi_ce = str_replace(doi_ce, "NA", NA_character_)) %>% 
  mutate(doi_url = str_replace(url, fixed(journal_doi_prefix[1]), "")) %>% 
  mutate(doi_url = str_replace(doi_url, fixed(journal_doi_prefix[2]), "")) %>% 
  mutate(doi_url = str_replace(doi_url, fixed(journal_doi_prefix[3]), "")) %>% 
  mutate(doi_url = str_replace(doi_url, fixed(journal_doi_prefix[4]), "")) %>% 
  mutate(doi_url = str_replace(doi_url, fixed(journal_doi_prefix[5]), "")) %>% 
  mutate(doi_url = str_replace(doi_url, fixed(journal_doi_prefix[6]), "")) %>% 
  mutate(doi_url = str_replace(doi_url, fixed(journal_doi_prefix[7]), "")) %>% 
  mutate(doi_url = str_replace(doi_url, fixed(journal_doi_prefix[8]), "")) %>% 
  mutate(doi_url = str_replace(doi_url, fixed(journal_doi_prefix[9]), "")) %>% 
  mutate(doi_url = str_replace(doi_url, fixed(journal_doi_prefix[10]), "")) %>% 
  mutate(doi_url = str_replace(doi_url, fixed(journal_doi_prefix[11]), "")) %>% 
  mutate(doi_url = str_replace(doi_url, fixed(journal_doi_prefix[12]), "")) %>% 
  mutate(doi_url = str_replace(doi_url, fixed(journal_doi_prefix[13]), "")) %>% 
  mutate(doi_url = str_replace(doi_url, fixed(journal_doi_prefix[14]), "")) %>% 
  mutate(doi_url = str_replace(doi_url, fixed(journal_doi_prefix[15]), "")) %>% 
  mutate(doi_url = str_replace(doi_url, fixed(journal_doi_prefix[16]), "")) %>% 
  mutate(doi_url = str_replace(doi_url, fixed(journal_doi_prefix[17]), "")) %>% 
  mutate(doi_url = str_replace(doi_url, fixed(journal_doi_prefix[18]), "")) %>% 
  mutate(doi_url = str_replace(doi_url, fixed(journal_doi_prefix[19]), "")) %>% 
  mutate(doi_url = str_replace(doi_url, fixed(journal_doi_prefix[20]), "")) %>% 
  mutate(doi_url = str_replace(doi_url, fixed(journal_doi_prefix[21]), "")) %>% 
  mutate(doi_url = str_replace(doi_url, fixed(journal_doi_prefix[22]), "")) %>% 
  mutate(doi_url = str_replace(doi_url, fixed(journal_doi_prefix[23]), "")) %>% 
  mutate(doi_url = str_replace(doi_url, fixed(journal_doi_prefix[24]), "")) %>% 
  mutate(doi_url = str_replace(doi_url, fixed(journal_doi_prefix[25]), "")) %>% 
  mutate(doi_url = str_replace(doi_url, fixed(journal_doi_prefix[26]), "")) %>% 
  mutate(doi_url = str_replace(doi_url, fixed(journal_doi_prefix[27]), "")) %>% 
  mutate(doi_url = str_replace(doi_url, fixed(journal_doi_prefix[28]), "")) %>% 
  mutate(doi_url = str_replace(doi_url, fixed(journal_doi_prefix[29]), "")) %>% 
  mutate(doi_url = str_replace(doi_url, fixed(journal_doi_prefix[30]), "")) %>% 
  mutate(doi_url = str_replace(doi_url, fixed(journal_doi_prefix[31]), "")) %>% 
  mutate(doi_url = str_replace(doi_url, doi_prefix[1], "")) %>%
  mutate(doi_url = str_replace(doi_url, doi_prefix[2], "")) %>% 
  mutate(doi_url = str_replace(doi_url, doi_prefix[3], "")) %>% 
  mutate(doi_url = str_replace(doi_url, doi_prefix[4], "")) %>% 
  mutate(doi_url = str_replace(doi_url, doi_prefix[5], "")) %>% 
  mutate(doi_url = str_replace(doi_url, doi_prefix[6], "")) %>% 
  mutate(doi_url = str_replace(doi_url, doi_prefix[7], "")) %>% 
  mutate(doi_url = str_replace(doi_url, doi_prefix[8], "")) %>% 
  mutate(doi_url = str_replace(doi_url, doi_prefix[9], "")) %>% 
  mutate(doi_url = str_replace(doi_url, "/abstract", "")) %>% 
  mutate(doi_url = str_replace(doi_url, "/full", "")) %>% 
  mutate(doi_url = str_replace(doi_url, "/pdf", "")) %>% 
  mutate(doi_url = str_replace(doi_url, ".pdf", "")) %>% 
  mutate(doi_url = str_replace(doi_url, "%2F", "/")) %>% 
  mutate(doi_url = str_replace(doi_url, "%2f", "/")) %>% 
  mutate(doi_url = str_replace(doi_url, fixed("&type=printable"), "")) %>% 
  mutate(doi_url = str_replace(doi_url, fixed("&partnerID=40&md5=e7e28a9bcf2a253ad4cdbc1c807581c1"), "")) %>% 
  mutate(doi_url = str_replace(doi_url, fixed("&partnerID=40&md5=69d21e627f92cd4b6f3c9837c45d6f2c"), "")) %>% 
  mutate(doi_url = str_replace(doi_url, fixed("&partnerID=40&md5=1af9d56b08f7404744b5e497fda4cd24"), "")) %>% 
  mutate(doi_url = str_replace(doi_url, fixed("&partnerID=40&md5=5c317266eef42c4e3e99d4ca26b2db29"), "")) -> cedat_study02
 
# birds
cebird %>%
  mutate(url = str_trim(url)) %>%
  mutate(doi_url = str_replace(url, fixed(journal_doi_prefix[1]), "")) %>% 
  mutate(doi_url = str_replace(doi_url, fixed(journal_doi_prefix[2]), "")) %>% 
  mutate(doi_url = str_replace(doi_url, fixed(journal_doi_prefix[3]), "")) %>% 
  mutate(doi_url = str_replace(doi_url, fixed(journal_doi_prefix[4]), "")) %>% 
  mutate(doi_url = str_replace(doi_url, fixed(journal_doi_prefix[5]), "")) %>% 
  mutate(doi_url = str_replace(doi_url, fixed(journal_doi_prefix[6]), "")) %>% 
  mutate(doi_url = str_replace(doi_url, fixed(journal_doi_prefix[7]), "")) %>% 
  mutate(doi_url = str_replace(doi_url, fixed(journal_doi_prefix[8]), "")) %>% 
  mutate(doi_url = str_replace(doi_url, fixed(journal_doi_prefix[9]), "")) %>% 
  mutate(doi_url = str_replace(doi_url, fixed(journal_doi_prefix[10]), "")) %>% 
  mutate(doi_url = str_replace(doi_url, fixed(journal_doi_prefix[11]), "")) %>% 
  mutate(doi_url = str_replace(doi_url, fixed(journal_doi_prefix[12]), "")) %>% 
  mutate(doi_url = str_replace(doi_url, fixed(journal_doi_prefix[13]), "")) %>% 
  mutate(doi_url = str_replace(doi_url, fixed(journal_doi_prefix[14]), "")) %>% 
  mutate(doi_url = str_replace(doi_url, fixed(journal_doi_prefix[15]), "")) %>% 
  mutate(doi_url = str_replace(doi_url, fixed(journal_doi_prefix[16]), "")) %>% 
  mutate(doi_url = str_replace(doi_url, fixed(journal_doi_prefix[17]), "")) %>% 
  mutate(doi_url = str_replace(doi_url, fixed(journal_doi_prefix[18]), "")) %>% 
  mutate(doi_url = str_replace(doi_url, fixed(journal_doi_prefix[19]), "")) %>% 
  mutate(doi_url = str_replace(doi_url, fixed(journal_doi_prefix[20]), "")) %>% 
  mutate(doi_url = str_replace(doi_url, fixed(journal_doi_prefix[21]), "")) %>% 
  mutate(doi_url = str_replace(doi_url, fixed(journal_doi_prefix[22]), "")) %>% 
  mutate(doi_url = str_replace(doi_url, fixed(journal_doi_prefix[23]), "")) %>% 
  mutate(doi_url = str_replace(doi_url, fixed(journal_doi_prefix[24]), "")) %>% 
  mutate(doi_url = str_replace(doi_url, fixed(journal_doi_prefix[25]), "")) %>% 
  mutate(doi_url = str_replace(doi_url, fixed(journal_doi_prefix[26]), "")) %>% 
  mutate(doi_url = str_replace(doi_url, fixed(journal_doi_prefix[27]), "")) %>% 
  mutate(doi_url = str_replace(doi_url, fixed(journal_doi_prefix[28]), "")) %>% 
  mutate(doi_url = str_replace(doi_url, fixed(journal_doi_prefix[29]), "")) %>% 
  mutate(doi_url = str_replace(doi_url, fixed(journal_doi_prefix[30]), "")) %>% 
  mutate(doi_url = str_replace(doi_url, fixed(journal_doi_prefix[31]), "")) %>% 
  mutate(doi_url = str_replace(doi_url, doi_prefix[1], "")) %>%
  mutate(doi_url = str_replace(doi_url, doi_prefix[2], "")) %>% 
  mutate(doi_url = str_replace(doi_url, doi_prefix[3], "")) %>% 
  mutate(doi_url = str_replace(doi_url, doi_prefix[4], "")) %>% 
  mutate(doi_url = str_replace(doi_url, doi_prefix[5], "")) %>% 
  mutate(doi_url = str_replace(doi_url, doi_prefix[6], "")) %>% 
  mutate(doi_url = str_replace(doi_url, doi_prefix[7], "")) %>% 
  mutate(doi_url = str_replace(doi_url, doi_prefix[8], "")) %>% 
  mutate(doi_url = str_replace(doi_url, doi_prefix[9], "")) %>% 
  mutate(doi_url = str_replace(doi_url, "/abstract", "")) %>% 
  mutate(doi_url = str_replace(doi_url, "/full", "")) %>% 
  mutate(doi_url = str_replace(doi_url, "/pdf", "")) %>% 
  mutate(doi_url = str_replace(doi_url, ".pdf", "")) %>% 
  mutate(doi_url = str_replace(doi_url, "%2F", "/")) %>% 
  mutate(doi_url = str_replace(doi_url, "%2f", "/")) %>% 
  mutate(doi_url = str_replace(doi_url, fixed("&type=printable"), "")) %>% 
  mutate(doi_url = str_replace(doi_url, fixed("&partnerID=40&md5=e7e28a9bcf2a253ad4cdbc1c807581c1"), "")) %>% 
  mutate(doi_url = str_replace(doi_url, fixed("&partnerID=40&md5=69d21e627f92cd4b6f3c9837c45d6f2c"), "")) %>% 
  mutate(doi_url = str_replace(doi_url, fixed("&partnerID=40&md5=1af9d56b08f7404744b5e497fda4cd24"), "")) %>% 
  mutate(doi_url = str_replace(doi_url, fixed("&partnerID=40&md5=5c317266eef42c4e3e99d4ca26b2db29"), "")) -> cebird02

# Manually include some missing dois
write.csv(cebird02, "data/cebird.csv", row.names = FALSE)
cebird03 <- read.csv("data/cebird.csv")

cedat_study02 %>% 
  mutate(url_flag = ifelse(str_detect(doi_url, "http"), 1, 0), 
         wos_flag = ifelse(str_detect(doi_url, "ISI"), 1, 0)) %>% 
  mutate(doi_url = ifelse(url_flag==1 | wos_flag==1, NA_character_, doi_url)) %>% 
  mutate(doi_clean = coalesce(doi_ce, doi_url)) %>% 
  filter(!is.na(doi_clean)) -> cedat_tidy
sum(cedat_tidy$url_flag) # 0
cedat_tidy %>% filter(!is.na(doi_clean)) %>% nrow() # 795

cebird03 %>% 
  mutate(url_flag = ifelse(str_detect(doi_url, "http"), 1, 0), 
         www_flag = ifelse(str_detect(doi_url, "www"), 1, 0), 
         wos_flag = ifelse(str_detect(doi_url, "ISI"), 1, 0)) %>% 
  mutate(doi_url = ifelse(url_flag==1 |www_flag==1 |wos_flag==1, NA_character_, doi_url)) %>% 
  mutate(doi_clean = doi_url) %>% 
  filter(!is.na(doi_clean)) -> cebird_tidy

# Now science direct (36 articles)
library(rscopus) # via UNIFAP
cedat_study02 %>% 
  mutate(sci_flag = ifelse(str_detect(doi_url, "pii"), 1, 0)) %>%
filter(sci_flag ==1) %>% pull(doi_url)

cedat_study02 %>% 
  mutate(sci_flag = ifelse(str_detect(doi_url, "pii"), 1, 0)) %>%
filter(is.na(doi_ce) ,sci_flag ==1) %>% 
  separate(doi_url, into=c("a", "id_pii"), sep="pii/") %>% 
  select(study.id, id_pii) -> els_toget

```

Now join with Abstracts from WOS search. 117 join at first attempt. 
127 after more dois tidied.
Test and develop searches......
```{r join-abstracts}
alt_all <- readRDS("data/alt_all.rds")
alt_all %>% 
  filter(!is.na(DI), PY>=2011) %>% 
  distinct() %>%
  group_by(SO) %>% summarise(count_articles = n()) %>% 
  arrange(desc(count_articles))
# Invalid doi
cedat_tidy %>% 
  separate(doi_clean, into=c("doi_clean", NA), sep=" ") %>%
  left_join(alt_all %>% select(DI, AB, SO, TC, altmetric), 
            by=c("doi_clean"="DI")) %>% 
  mutate(flag_na = ifelse(is.na(TC),1,0)) %>% 
  filter(! doi_clean =="10.7325/7") %>% 
  mutate(flag_search = if_else(is.na(SO),0,1), 
         flag_ce = 1) -> cedat_alt
# check doi is unique (max should be 1)
cedat_alt %>% 
  group_by(doi_clean) %>% summarise(count_articles = n()) %>% 
  arrange(desc(count_articles)) %>% pull(count_articles) %>% max()

cedat_alt %>% 
  group_by(SO) %>% summarise(count_na = sum(flag_na)) %>% 
  arrange(desc(count_na))

cedat_alt %>% 
  filter(!is.na(SO)) #127
# Find CE articles missed by my search
cedat_alt %>%
  filter(!is.na(journal.name)) %>%
  group_by(journal.name) %>% 
    summarise(count_articles = length(unique(study.id)), 
              count_both = sum(flag_search)) %>% 
  arrange(desc(count_articles))

```

Add Journals and Abstracts using crossref and pubmed.
```{r abstracts-crossref}
# crossref
# Politely import Abstracts 
# 1) helper functions
source("R/safe_cr_cn.R")
source("R/get_cr_cn.R")
source("R/safe_abstracts.R")
source("R/get_abstracts.R")
# Journals to go with dois
out_crossref <- rcrossref::cr_cn(dois = "10.1016/j.biocon.2015.08.034", 
                                 format = "bibentry")
# Journal names (so can double check CE)
j01 <- plyr::ddply(cedat_alt[1:200, ], .(doi_clean), .fun = get_cr_cn)
j02 <- plyr::ddply(cedat_alt[201:400, ], .(doi_clean), .fun = get_cr_cn)
j03 <- plyr::ddply(cedat_alt[401:600, ], .(doi_clean), .fun = get_cr_cn)
j04 <- plyr::ddply(cedat_alt[601:795, ], .(doi_clean), .fun = get_cr_cn) 

# stick together
bind_rows(j01, j02, j03, j04) %>% 
  mutate(journal = str_replace_all(journal, "[^[:alnum:]]", " ")) %>% 
  mutate(journal = str_replace(journal, "amp  mathsemicolon", "")) %>% 
  mutate(journal = str_replace(journal, "Journal du Conseil", "")) %>%
  mutate(journal = str_to_upper(str_squish(str_trim(journal)))) -> journal_names
# check doi is unique (max should be 1)
cedat_alt %>% 
  group_by(doi_clean) %>% summarise(count_journals = n()) %>% 
  arrange(desc(count_journals)) %>% pull(count_journals) %>% max()
# 9 missing with doi
cedat_alt %>% left_join(journal_names) %>% filter(is.na(journal))
# Abstracts
"10.1111/acv.12189"
t <- safe_abstracts(doi="10.1111/acv.12189")
ab01 <- plyr::adply(cedat_alt[1:200, ], .margins = 1, .fun = get_abstracts)
ab02 <- plyr::adply(cedat_alt[201:400, ], .margins = 1, .fun = get_abstracts)
ab03 <- plyr::adply(cedat_alt[401:600, ], .margins = 1, .fun = get_abstracts)
ab04 <- plyr::adply(cedat_alt[601:795, ], .margins = 1, .fun = get_abstracts)

# stick together
bind_rows(ab01, ab02, ab03, ab04) %>% 
  mutate(ab_text = 
           str_to_upper(str_trim((str_replace(ab_text, "Abstract", ""))))) %>% 
  mutate(ab_clean = coalesce(AB, ab_text)) -> abs_crossref
# 233 with Abstracts
abs_crossref %>% 
 filter(!is.na(ab_clean))

#Pubmed
#library(pubmedR) #Error: HTTP failure: 400
library(easyPubMed)
library(bibliometrix)
#My API: ff5578bfd85d5a8281f602b4b6b9f1cc7e08
api_key <- "ff5578bfd85d5a8281f602b4b6b9f1cc7e08"
query <- "10.1016/j.biocon.2015.08.034[DOI]"
query2 <- "10.1111/acv.12189[DOI]"
my_query <- safe_pubmedid(query2, 
                          api_key ="ff5578bfd85d5a8281f602b4b6b9f1cc7e08")
# easypubmed
source("R/safe_pubmedid.R")
source("R/get_pubmedids.R")
pubmed_abs01 <- plyr::ddply(cedat_alt[1:100, ], .(doi_clean), .fun = get_pubmedids)
pubmed_abs02 <- plyr::ddply(cedat_alt[101:200, ], .(doi_clean), .fun = get_pubmedids)
pubmed_abs03 <- plyr::ddply(cedat_alt[201:400, ], .(doi_clean), .fun = get_pubmedids)
pubmed_abs04 <- plyr::ddply(cedat_alt[401:450, ], .(doi_clean), .fun = get_pubmedids)
pubmed_abs05 <- plyr::ddply(cedat_alt[451:475, ], .(doi_clean), .fun = get_pubmedids)
pubmed_abs06 <- plyr::ddply(cedat_alt[476:500, ], .(doi_clean), .fun = get_pubmedids)
pubmed_abs07 <- plyr::ddply(cedat_alt[501:550, ], .(doi_clean), .fun = get_pubmedids)
pubmed_abs08 <- plyr::ddply(cedat_alt[551:600, ], .(doi_clean), .fun = get_pubmedids)
pubmed_abs09 <- plyr::ddply(cedat_alt[601:650, ], .(doi_clean), .fun = get_pubmedids)
pubmed_abs10 <- plyr::ddply(cedat_alt[651:700, ], .(doi_clean), .fun = get_pubmedids)
pubmed_abs11 <- plyr::ddply(cedat_alt[701:750, ], .(doi_clean), .fun = get_pubmedids) 
pubmed_abs12 <- plyr::ddply(cedat_alt[751:795, ], .(doi_clean), .fun = get_pubmedids)
# stick together 795 to 775 with distinct
bind_rows(pubmed_abs01, pubmed_abs02, pubmed_abs03, pubmed_abs04, pubmed_abs05, 
          pubmed_abs06, pubmed_abs07, pubmed_abs08, pubmed_abs09, pubmed_abs10, 
          pubmed_abs11, pubmed_abs12) %>%   distinct() %>% 
  mutate(ab_pubmed = str_to_upper(str_trim(ab_pubmed))) -> abs_pub
# birds 
j05 <- plyr::ddply(cebird_tidy, .(doi_clean), .fun = get_cr_cn) 
ab05 <- plyr::adply(cebird_tidy, .margins = 1, .fun = get_abstracts)
pubmed_abs13 <- plyr::ddply(cebird_tidy, .(doi_clean), .fun = get_pubmedids)
# stick together. Only 2 Abstracts.
cebird_tidy %>% left_join(ab05) %>% left_join(pubmed_abs13) %>% 
  left_join(alt_all %>% select(DI, AB), by=c("doi_clean"="DI")) -> abs_brids
# Dimensions
library(dimensionsR)
token <- dsAuth(username = "darren.norris@unifap.br", password = "Dimensions@78")

# rscopus
# DOI("doi1") OR ("doi2") OR...

abs_crossref %>% left_join(abs_pub) %>% 
  mutate(ab_clean = str_squish(str_trim(coalesce(AB, ab_pubmed, ab_text)))) %>% 
  left_join(journal_names) %>% 
  mutate(journal_name = coalesce(SO, journal)) -> abs_all
# 280 Abstracts
abs_all %>% 
  filter(!is.na(ab_clean)) 
# which journals have most missing abstracts
abs_all %>% 
  filter(is.na(ab_clean)) %>% 
  group_by(journal_name) %>% 
  summarise(acount = n()) %>% arrange(desc(acount))
#Unclear why missing Abstracts from same journal
abs_all %>% 
  filter(is.na(ab_clean)) %>%
  filter(journal_name =="ANIMAL CONSERVATION")

#Export. Need to manually clean few Abstracts starting with refs.
cedat_alt %>% pull(study.id) -> sids
cedat_studies %>% 
  left_join(abs_all %>% filter(study.id %in% sids) %>% 
              select(study.id, doi_clean, ab_clean)) %>% 
  mutate(doi_url = if_else(is.na(doi_clean), doi_clean, paste("https://doi.org/", doi_clean, sep=""))) %>% 
  select(study.id, publication.year,publication.title, journal.name, doi, url, doi_clean, doi_url, ab_clean, intervention_count) -> out_cedat_dois
# 280 Abstracts
out_cedat_dois %>% filter(!is.na(ab_clean)) %>% nrow()
# Find which to update/tidy more
out_cedat_dois %>% 
  filter(!is.na(journal.name)) %>%
  mutate(flag_na_doi = if_else(is.na(doi_clean),1,0), 
         flag_na_ab = if_else(is.na(doi_clean),1,0)) %>% 
  group_by(journal.name) %>% 
  summarise(tot_articles = length(unique(study.id)), 
            na_doi = sum(flag_na_doi), 
            na_ab = sum(flag_na_ab)) %>% 
  arrange(desc(na_doi)) %>% 
  filter(na_doi > 0, na_doi < tot_articles) -> cedat_tocheck
#compare with original data
out_cedat_dois %>% 
  filter(!is.na(journal.name), !is.na(doi_clean)) %>% 
  group_by(journal.name) %>% summarise(article_count = n()) %>% 
  arrange(desc(article_count))

#out_cedat_dois  %>% 
#  write.csv("data/cedat_cleandois.csv", row.names = FALSE) 
  
```


Export to get Zhang search results
```{r export-dois}
bind_rows(
abs_all %>% filter(!is.na(doi_clean)) %>% select(study.id, doi_clean) %>% 
  mutate(source="cedat"),
abs_brids %>% filter(!is.na(doi_clean)) %>% select(ID, doi_clean) %>% 
  rename("study.id"="ID") %>% mutate(source="cebirds")
) %>% select(source, study.id, doi_clean) %>% 
  write_rds("data/ce_dois.RDS")
```

Add Zhang Abstracts and export final.
```{r}
zhang_abs <- readRDS("data/zhang_abss.RDS")

cedat_studies %>% 
  left_join( 
    abs_all %>% 
              left_join(zhang_abs %>% select(study.id, AB) %>% distinct() %>%
                        rename("ABzhang"="AB") %>% 
                          mutate(ABzhang = str_replace(ABzhang, 
                                                       fixed("[NO ABSTRACT AVAILABLE]"), NA_character_))) %>%
  mutate(ab_clean = str_to_upper(coalesce(AB, ABzhang, ab_pubmed, ab_text))) %>% 
    filter(study.id %in% sids) %>% 
              select(study.id, doi_clean, ab_clean)) %>%
  mutate(doi_url = if_else(is.na(doi_clean), doi_clean, paste("https://doi.org/", doi_clean, sep=""))) %>% 
  #filter(!is.na(ab_clean))  #507 abstracts
select(study.id, publication.year,publication.title, journal.name, doi, url, doi_clean, doi_url, ab_clean, intervention_count) -> t1

cebird %>% 
  left_join(abs_brids %>% 
  rename("study.id"="ID") %>% 
    left_join(zhang_abs %>% select(study.id, AB) %>% distinct() %>%
                        rename("ABzhang"="AB")) %>% 
           mutate(ABzhang = str_replace(ABzhang, 
                                        fixed("[NO ABSTRACT AVAILABLE]"), NA_character_)) %>% 
  mutate(ab_clean = str_to_upper(coalesce(AB, ABzhang, ab_pubmed, ab_text)), 
         study.id = as.character(study.id)) %>% 
    select(study.id, doi_clean, ab_clean), 
  by= c("ID"="study.id")) %>% 
  select(ID, year, title, journal, url, doi_clean, ab_clean) %>% 
  mutate(doi_url = if_else(is.na(doi_clean), doi_clean, paste("https://doi.org/", doi_clean, sep=""))) %>% 
  rename("study.id"="ID", 
         "publication.year"="year", 
         "publication.title"="title", 
         "journal.name"="journal") %>% 
  mutate(study.id = as.numeric(study.id), 
         publication.year = as.numeric(publication.year), 
         doi=NA, 
         intervention_count=NA) %>% 
  select(study.id, publication.year,publication.title, journal.name, doi, url, doi_clean, doi_url, 
ab_clean, intervention_count) -> t2

#Make sure no duplicates
t1 %>% pull(study.id) %>% unique() -> t1_studies
t2 %>% 
  filter(!study.id %in% t1_studies) -> t2
# bind together
bind_rows(t1, t2) %>% 
  mutate(ab_clean = str_squish(str_trim(str_to_upper(ab_clean)))) -> out_doi_ab
# how many?
out_doi_ab %>% filter(!is.na(doi_clean)) %>% nrow() #805 DOIS
out_doi_ab %>% filter(!is.na(ab_clean)) %>% nrow() # 516 Abstracts
# Export  
#out_doi_ab %>% write.csv("data/cedat_cleandois.csv", row.names = FALSE) 

```

Save workspace.
```{r }
save.image("data/ce_abstracts.RData")
```

Get missing Abstracts from Scopus
Use here to test API key: 
https://dev.elsevier.com/scopus.html
```{r scopus-abstracts}

s <- read_excel("data/something.xlsx")
s %>% filter(name=="apikey") %>% pull(value) -> myapikey
options("elsevier_api_key" = myapikey)
have_api_key()
print(rscopus::get_api_key(), reveal = TRUE)
s %>% filter(name=="insttoken") %>% pull(value) -> myinsttoken
insttoken <- myinsttoken
insttoken <- inst_token_header(insttoken)


# 136 missing from Elsevier publications
out_doi_ab %>% 
  mutate(sci_flag = ifelse(str_detect(url, "pii"), 1, 0)) %>%
filter(sci_flag ==1, is.na(ab_clean)) # 136

out_doi_ab %>% 
  mutate(sci_flag = ifelse(str_detect(url, "pii"), 1, 0)) %>%
filter(is.na(ab_clean) ,sci_flag ==1) %>% 
  separate(url, into=c("a", "id_pii"), sep="pii/") %>% 
  select(study.id, id_pii) -> els_toget

# Abstracts and doi from pii
 x = abstract_retrieval("S1439179112000151", identifier = "pii",
   verbose = FALSE, headers = insttoken)
 # Abstract text
 x$content$`abstracts-retrieval-response`$coredata$`dc:description`
 # Doi
 x$content$`abstracts-retrieval-response`$coredata$`prism:doi`
 
# helper functions
source("R/safe_scopus_ab.R")
source("R/get_scopus_ab.R")
 t1 <- els_toget[1, ]
 alt_res <-  safe_scopus_ab(id = t1$id_pii, identifier = "pii",
                             verbose = FALSE, headers = insttoken) 
 
 
ab_s01 <- plyr::adply(els_toget[1:75, ], .margins = 1, .fun = get_scopus_ab)
ab_s02 <- plyr::adply(els_toget[76:136, ], .margins = 1, .fun = get_scopus_ab)
bind_rows(ab_s01, ab_s02) %>% 
  mutate(ab_scopus = str_to_upper(str_squish(str_trim(ab_scopus)))) -> scopus_abs

 out_doi_ab %>% left_join(scopus_abs) %>% 
   mutate(doi_clean = coalesce(doi_clean, doi_scopus), 
          ab_clean = coalesce(ab_clean, ab_scopus)) %>% 
   mutate(doi_url = if_else(is.na(doi_clean), doi_clean, paste("https://doi.org/", doi_clean, sep=""))) %>% 
   select(study.id, publication.year,publication.title, journal.name, doi, url, doi_clean, doi_url, ab_clean, intervention_count) -> out_doi_ab_scopus
 # how many?
out_doi_ab_scopus %>% filter(!is.na(doi_clean)) %>% nrow() #843 DOIS
out_doi_ab_scopus %>% filter(!is.na(ab_clean)) %>% nrow() # 653 Abstracts

# Export  
out_doi_ab_scopus %>% write.csv("data/cedat_cleandois.csv", row.names = FALSE) 
   
```

Now for dois missing Abstracts
```{r scopus-dois}

# 136 missing from Elsevier publications
out_doi_ab_scopus %>% 
filter(!is.na(doi_clean), is.na(ab_clean)) # 190

out_doi_ab_scopus %>% 
  filter(!is.na(doi_clean), is.na(ab_clean), !is.na(journal.name)) %>% 
  select(study.id, doi_clean) -> scopus_toget

source("R/get_scopus_abdoi.R")
# error to sort see scopus_toget[50, ] scopus_toget[50, ]....
t1 <- get_scopus_abdoi(scopus_toget[50, ])
alt_res <-  safe_scopus_ab(id = scopus_toget[50, ]$doi_clean, identifier = "doi",
                             verbose = FALSE, headers = insttoken) 

dfout <- data.frame(ab_scopus = alt_res$result$content$`abstracts-retrieval-response`$coredata$`dc:description`, 
                        doi_scopus = alt_res$result$content$`abstracts-retrieval-response`$coredata$`prism:doi`)

ab_s03 <- plyr::adply(scopus_toget[1:50, ], .margins = 1, .fun = get_scopus_abdoi)
ab_s07 <- plyr::adply(scopus_toget[51:75, ], .margins = 1, .fun = get_scopus_abdoi)
ab_s08 <- plyr::adply(scopus_toget[76:80, ], .margins = 1, .fun = get_scopus_abdoi)
ab_s09 <- plyr::adply(scopus_toget[81:91, ], .margins = 1, .fun = get_scopus_abdoi)
ab_s10 <- plyr::adply(scopus_toget[93:150, ], .margins = 1, .fun = get_scopus_abdoi)
ab_s11 <- plyr::adply(scopus_toget[151:190, ], .margins = 1, .fun = get_scopus_abdoi)

bind_rows(ab_s03, ab_s04, ab_s05, ab_s06, ab_s07, ab_s08, 
          ab_s09, ab_s10, ab_s11) %>% 
  mutate(ab_scopus = str_to_upper(str_squish(str_trim(ab_scopus)))) -> scopus_abs02

out_doi_ab_scopus %>% left_join(scopus_abs02) %>% 
   mutate(doi_clean = coalesce(doi_clean, doi_scopus), 
          ab_clean = coalesce(ab_clean, ab_scopus)) %>% 
   mutate(doi_url = if_else(is.na(doi_clean), doi_clean, paste("https://doi.org/", doi_clean, sep=""))) %>% 
   select(study.id, publication.year,publication.title, journal.name, doi, url, doi_clean, doi_url, ab_clean, intervention_count) -> out_doi_ab_scopus02
 # how many?
out_doi_ab_scopus02 %>% filter(!is.na(doi_clean)) %>% nrow() # 842 DOIS
out_doi_ab_scopus02 %>% filter(!is.na(ab_clean)) %>% nrow() # 827 Abstracts

# Export  
out_doi_ab_scopus02 %>% write.csv("data/cedat_cleandois.csv", row.names = FALSE) 

```

double check results.
```{r check-results}
cedat_cleandois <- read.csv("data/cedat_cleandois.csv", as.is=TRUE) %>% 
  filter(!is.na(study.id))
# Export  
cedat_cleandois %>% write.csv("data/cedat_cleandois.csv", row.names = FALSE) 
cedat_cleandois %>%  filter(!is.na(journal.name)) %>% nrow() #1107
cedat_cleandois %>% filter(!is.na(doi_clean)) %>% nrow() # 842 DOIS
cedat_cleandois %>% filter(!is.na(ab_clean)) %>% nrow() # 827 Abstracts
# 247 without doi that should be avaiable via scopus
cedat_cleandois %>% 
  filter(!is.na(journal.name)) %>% 
  mutate(flag_na_doi = if_else(is.na(doi_clean),1,0), 
         flag_na_ab = if_else(is.na(doi_clean),1,0)) %>% 
  group_by(journal.name) %>% 
  summarise(tot_articles = length(unique(study.id)), 
            na_doi = sum(flag_na_doi), 
            na_ab = sum(flag_na_ab)) %>% 
  arrange(desc(na_doi)) %>% 
  ungroup() %>%
  filter(na_doi > 0, na_doi < tot_articles) %>% 
  arrange(desc(tot_articles)) #%>% pull(tot_articles) %>% sum()

cedat_cleandois %>% 
  group_by(study.id) %>% summarise(count_studies =n()) %>%
  arrange(desc(count_studies))

#827 distinct Abstracts
cedat_cleandois %>% filter(!is.na(ab_clean)) %>% select(ab_clean) %>% 
  distinct()

# export dois to get
cedat_cleandois %>% 
  filter(!is.na(journal.name)) %>% 
  mutate(flag_na_doi = if_else(is.na(doi_clean),1,0), 
         flag_na_ab = if_else(is.na(doi_clean),1,0)) %>% 
  group_by(journal.name) %>% 
  summarise(tot_articles = length(unique(study.id)), 
            na_doi = sum(flag_na_doi), 
            na_ab = sum(flag_na_ab)) %>% 
  arrange(desc(na_doi)) %>% 
  ungroup() %>%
  filter(na_doi > 0, na_doi < tot_articles) %>% 
  pull(journal.name) -> missing_journals

cedat_cleandois %>% 
  filter(journal.name %in% missing_journals, is.na(doi_clean)) -> 
  missing_dois
write.csv(missing_dois, "data/missing_dois.csv", row.names = FALSE)
  

```

